Title         : Navigating the Universe of Z3 Theory Solvers
Author        : Nikolaj Bj&oslash;rner
Affiliation   : Microsoft Research
Author        : Lev Nachmanson
Affiliation   : Microsoft Research
Bibliography  : refs.bib


Colorizer     : python
Bib style     : plainnat
Bibliography  : example
Logo          : True

Doc class     : llncs

Package       : url
Package       : amssymb
Package       : [curve]xypic
Package       : tikz
Package       : algorithm2e
Package       : bussproofs
~Exercise     : @h1-exercise=lower-case @h1-exercise label="@h1@h1-exercise"
                margin-left=0em
                before="[**Exercise &label;: **]{margin-left=0em}&br;"

~ MathDefs
\newcommand{\Mbp}{Mbp}
\newcommand{\pre}{pre}
\newcommand{\true}{\mathit{true}}
\newcommand{\false}{\mathit{false}}
\newcommand{\safe}{\mathit{Safe}}
\newcommand{\Lit}{\mathcal{L}}
\newcommand{\Model}{\mathsf{M}}
\newcommand{\dbar}{\,|\!|\,}
\newcommand{\searchstate}[2]{#1 \dbar #2}
\newcommand{\conflstate}[3]{#1 \dbar #2 \dbar #3}
\newcommand{\nsb}[1]{[\emph{Nikolaj:} #1]}
\newcommand{\papercomment}[1]{}
\newcommand{\onenorm}[1]{|\!|#1|\!|_1}
\newcommand{\Th}{{T}}
\newcommand{\relaxOp}{\mathit{relax}}
\newcommand{\restrictOp}{\mathit{restrict}}
\newcommand{\Decision}[1]{#1^\delta}
\newcommand{\Propagation}[2]{#1^{#2}}
\newcommand{\Theory}{\mathit{Theory}}
~

[TITLE]


~ Abstract
Modular combination of little engines of proof is an integral theme
in engineering modern SMT solvers. The CDCL(T) architecture provides
an overall setting for how theory solvers may cooperate
around a SAT solver based on conflict driven clause learning.
The Nelson-Oppen framework provides the interface contracts between
theory solvers for disjoint signatures. In this paper we review principles
of theory integration in CDCL(T) and then examine
the theory solvers available in Z3, how they integrate, feedback from 
use scenarios, and we reflect on lessons derived from the integration.
~

[TOC]

# Introduction { #sec-intro }

The aim of this paper is to provide an up-to-date overview of Z3's core solver engines.
While some of the engines date back quite some time, other engines have been 
replaced over time, added, and some engines added and then removed. We here provide
a first description of the underlying principles of some of the engines that is
not documented elsewhere. As an overview paper, we will not be able to go into
depth into any one of the engines, but hopefully provide enough of an idea
of each of the approaches.

We will apply the following taxonomy when discussing the theory solvers.
It extends the reduction approach to decision procedures [@KapurZarbaReduction]
as explained in the context of Z3 in [@MouraB09].


* Boolean Theories 
  * Domains that are inherently finite domain and can be solved by reducing to an underlying CDCL engine.
  * Instances: Bit-vectors, Cardinality and Pseudo-Boolean constraints
* Base theories 
  * Theories that form a basis for all other theories.
  * Instances: the theory of uninterpreted functions and the theory of arithmetic
* Reducible theories 
  * Theories that can be solved by reducing into base theories.
  * Instances: Arrays, IEEE floating points, algebraic datatypes, recursive functions
* Hybrid theories 
  * Theories that combine non-disjoint signatures from different theories.
  * Instances: Sequences, Model-based quantifier instantiation
* External theories 
  * Theories that may be developed externally by propagating consequences and identifying conflicts.
  * Instance: an external propagator plugin

## Present and Future
Z3 has now in fact two CDCL(T) solvers. For main SMT workloads it offers a CDCL(T) core
that integrates all the theories that we mention here. This core is a continuation
of the main solver of Z3 since its inception. A core with near up-to-date advances in SAT
solving has been used so far for workloads originating from bit-vector and Pseudo-Boolean constraints.
This core is currently being updated to handle most if not all the SMT workloads of the legacy
core with the intent of modernizing Z3's core and allow integrating new techniques around
in-processing, logging of clausal proofs, model repair, and other goodies.
From the perspective we give in this paper, we consider these cores the same.
Z3 exposes several other core solvers that either build on top of the SMT solver or entirely 
bypass it. For Horn clauses, Z3 contains dedicated engines for finite domain Horn clauses using finite hash-tables, 
and for Constrained Horn Clauses (CHC) it uses the SPACER [@KomuravelliGCC13] solver; for quantifier-free formulas
over Tarski's fragment of polynomial arithmetic it exposes a self-contained solver; and for quantified
formulas for theories that admit quantifier-elimination it exposes self-contained solvers.


# CDCL(T) - In the light of Theory Solvers

We here recall the main mechanisms used in mainstream modern SAT solvers in the light of theory solving.
When SAT solving, as implemented using conflict driven clause learning, CDCL, is combined with theory solving
it augments propositional satisfiability with theory reasoning. The CDCL solver maintains a set of 
formulas $F$ and a partial assignment to literals in $F$ that we refer to as $M$.  
The solver starts in a state $\langle M, F\rangle$, where $M$ is initially empty. 
It then attempts to complete $M$ to a full model of $F$ to show that $F$ is satisfiable
and at the same time addes consequences
to $F$ to establish that $F$ is unsatisfiable. The transition between the search for a satisfying
solution and a consequence is handled by a _conflict resolution_ phase. 
The state during conflict resolution is a triple $\langle M, F, C\rangle$, 
where, besides the partial model $M$ and formula $F$, there is also a conflict clause $C$,
that is false under $M$.
The auxiliary function _Theory_ is used to advance decisions, propagations and identify conflicts.
If _Theory_ determines that $S$ is conflicting with respect to the literals in $M$ it produces a conflict clause $C$, that
contains a subset of conflicting literals from $M$. It can also produce a trail assignment $A$, which is either a propagation
or decision and finally, if it determines that $S$ is satisfiable under trail $M$ it produces $SAT$.

From the point of view of the CDCL(T) solver
theory reasoning is a module that can take a state during search and produce verdicts on how search should progress.
We use the following verdicts of a theory invocation $\Theory(M,F)$:

* $SAT$. The theory solver may determine that the assignment $M$ extendes to a model of $F$.
* Conflict $C$. The theory solver may determine that a subset $M$ is inconsistent relative to $F$. 
  In the propositional case an inconsistent clause $C$ is a member of $F$, such that each literal in $C$ is false in $M$.
  With theory reasoning, $C$ does not need to correspond to a clause in $F$, but be assignments in $M$ that are inconsistent modulo theories.
* A propagation $\Propagation{\ell}{C}$. The theory solver propagates a literal $\ell$.
* A decision $\Decision{\ell}$. 

the partial model extends to a model of the theories, can identify a subset of $M$ as an unsatisfiable core, 
propagate the truth assignment of a literal $\ell$, or create a new case split $\Decision{\ell}$ for a
literal $\ell$ that has not already been assigned in $M$.


~MathPre
\langle M, {F} \rangle                      & \Rightarrow & SAT                                                    & SAT = \Theory(M, F)

\langle M, F \rangle                        & \Rightarrow & \langle M, C, F \rangle                                & C = \Theory(M, F) 

\langle M, {F} \rangle                      & \Rightarrow & \langle M A, F \rangle                                 & A = \Theory(M, F) 

\langle M, \emptyset, {F} \rangle           & \Rightarrow & UNSAT 

\langle M \Decision{\overline{\ell}}, {F}, C \rangle & \Rightarrow & \langle M \Propagation{\ell}{C}, {F} \rangle         & \ell \in {C}

\langle M, F \Propagation{\ell}{C'}, {C}\rangle     & \Rightarrow & \langle M, F, (C \setminus \{\overline{\ell}\}) \cup (C'\setminus\{{\ell}\}) \rangle & \overline{\ell} \in {C}

\langle M A, F, C \rangle               & \Rightarrow & \langle M, F, {C} \rangle                              & otherwise

~



## Invariants

To be well-behaved we expect _Theory_ to produce propagations on literals that don't already appear in $M$, and crucially
enforce the main invariants:

* The conflict clause $C$ is false in $M$ and a consequence of $F$. Thus, 
  for state $\langle M, F, C \rangle$ we have $F \models_T \bigvee_{\ell \in C} \neg \ell$, as well as $\overline{C} \in M$.
* A propagated literal is justified by the current partial model $M$. Thus, 
  for state $\langle M \Propagation{\ell}{C}, F \rangle$ we have $F \models_T C$, $\ell \in C$, 
  and for each $\ell' \in C \setminus \{ \ell \}: \ell' \in M$.

That is, each conflict clause is a consequence of $F$ and each propagation is also a consequence of $F$, and the premises of a propagation is justified by $T$.


# Boolean Theories

## Bit-vectors

## Pseudo-Booleans

# Base Theories
## Uninterpreted Functions 
 
## Arithmetic 

There are several alternate engines for arithmetical constraints in Z3. 
Some of the engines are engineered for fragments of arithmetic, such
as difference arithmetic, where all inequalities are of the form $x - y \leq k$, 
for $k$ a constant, and unit-two-variable-per-inequality (UTVPI), where
all inequalities are of the form $\pm x \pm y \leq k$. A new main solver
for general arithmetic formulas has emerged recently, with the longer term
objective of entirely replacing Z3's legacy arithmetic solver. We will here
describe internals of the newer solver in more detail.

In overview, the arithmetic solver uses a waterfall model for solving arithmetic constraints.

* First it establishes feasibility with respect to linear inequalities. Variables are solved over the rationals.
* Second, it establishes feasibility with respect to mixed integer linear constraints. Integer variables are solved if they are assigned integer values.
* Finally, it establishes feasibility with respect to non-linear polynomial constraints.

### Rational linear arithmetic

The solver for rational linear inequalities uses a dual simplex solver as explained in [@DutertreM06].
It maintains a global set of equalities of the form $A\vec{x} = 0$, and
each variable $x_j$ is assigned lower and upper bounds during search.
The solver then checks for feasibility of the resulting system
$A\vec{x} = 0, lo_j \leq x_j \leq hi_j, \forall j$ for dynamically
changing bounds $lo_j, hi_j$.
The bounds are _justified_ by assignments in $M$. 


#### Finding equal variables - cheaply

The new arithmetic solver contains a novel, efficient, method for finding pairs of variables
that are forced equal. By finding such variable, the arithmetic solver can then propagate
equalities to other theories. Such equality propagation can make a big difference for 
efficiently integrating arithmetic in hybrid theories, such as the theory of sequences.
We will in the following outline the new idea.

A starting point is a tableau with bounds $A\vec{x} = 0, lo_j \leq x_j \leq hi_j, \forall j$.
The tableau contains a sub-tableau of equalities where at most two variables are non-fixed
in every equation, such that the coefficients of the non-fixed variables have the same
absolute value. 
That is, it contains a system $S$ be a system of linear equations of the form 
$e_i:=a_{i} x_{i} + b_{i} y_{i}=c_i$, where $x_{i},y_{i}$ are variables and
 $c_i$ is a constant, and $a_i, b_i \in \{-1,1\}$ for $0 \le i < n$. 
It is relatively cheap to detect whether a row in a tableau corresponds
to such an equation: it can have at most two non-fixed variables whose coefficients
must have the same absolute values.
We assume that the variable values and the constants are rational numbers.
For bounds propagation it is only relevant to consider feasible tableaux. 
So $S$ has a solution $V$, so we have
 $a_{i} V(x_{i})+ b_{i} V(y_{i})=c_i$ for every $i$. Let us call
 variables $u$ and $v$ equivalent if in every solution of $S$ they
 have the same values.

Given $S$ and $V$, we provide an efficient
algorithm for finding pairs of variables $(u, t)$ such that $u$ is equivalent
to $t$. We will formulate the result more precisely later.

Let us start from an example. Consider the following system
~MathPre
x - y = 3, &    y - w = 2, &    w + t = 4
y + z = 3, &    z + u = 4, &    v - z = 2, t - s = 1
~
 and solution

~MathPre
V:  x \rightarrow 5, y \rightarrow 2, z \rightarrow 1, u \rightarrow 3, w \rightarrow 0, s \rightarrow 3, t \rightarrow 4, v \rightarrow 3
~

 By examining $V$, we might suspect that the
 $u,v$, and $s$ are pairwise equivalent. However, if we increase $x$ by one
 we have another solution
~MathPre
V':  x \rightarrow 6, y \rightarrow 3, z \rightarrow 0, u \rightarrow 4, w \rightarrow 1, s \rightarrow 2, t \rightarrow 3, v \rightarrow 2
~
 Since $V'(u) \ne V'(v)$, we can conclude that $u$ is not equivalent to
 either $s$ or $v$. But we can prove that $v$ and $s$ are equivalent.
Another observation is that each variable changed its
value by $\pm 1$: changing as $x$ or as $-x$. 

We introduce our algorithm by recalling which inference rules it simulates, without 
actually literally performing the inferences. The inferences are

~Math
\begin{array}{ccc}
\AxiomC{$x - y = c_1$}
\AxiomC{$y - z = c_2$}
\BinaryInfC{$x - z = c_1 + c_2$}
\DisplayProof
&
\AxiomC{$x + y = c_1$}
\AxiomC{$z - y = c_2$}
\BinaryInfC{$x + z = c_1 + c_2$}
\DisplayProof
&
\AxiomC{$x + y = c_1$}
\AxiomC{$z + y = c_2$}
\BinaryInfC{$x - z = c_1 - c_2$}
\DisplayProof
\end{array}
~
 
The goal of deriving implied equalities is achieved if we can infer an equality $x - y = 0$ 
using the above inference rules.
Now, the crucial observation is that there is no need to perform arithmetic on the constant offsets
$c_i$ because we have a valuation $V$ and the property for every derived equation $x - y = c$:

~Math
   c = 0  \Leftrightarrow V(x) = V(y)
~
So it is enough to look at the variables $x, y$ and check if their values are the same for a derived equality.
The benefit of avoiding arithmetic on the coefficients can be signficant when the coefficients cannot be 
represented using ordinary 64-bit registers, but require arbitrary precision arithmetic.
Note that the same method also detects fixed variables; whenever deriving $x + x = c$, we have proved
$x$ to be fixed at value $c/2$. In this case, the method needs to calculate $c$. All other variables connected
to $x$ through binary equalities are naturally also fixed.

To implement search over the derivation rules efficiently, we build a depth-first search
forest where the nodes are annotated by variables annotated by signs and their values from $V$.
Edges are annotated by octagon equations. 
The forest is built by picking a so-far unprocessed variable and growing a tree under it
by traversing all equations involving the variable. During the tree expansion the algorithm
checks if two nodes with the same _value_ and same _signs_ are connected.
To propagate a new equality we trace back the path that was used to derive the equality and assemble the justifications from each row.

~Example
Let us show how the procedure works on the example from above and valuation $V$. 
Starting with the node $\langle +t, 4 \rangle$, the equation $t - s = 1$ 
produces the children $\langle +s, 3\rangle$ and $\langle -w, 0\rangle$.
The second child can be expanded in turn into 
$\langle -y, -2 \rangle$, $\langle z, 1\rangle$, $\langle v, 3\rangle$.
This establishes a path from $\langle s, 3\rangle$ to $\langle v, 3\rangle$. 
The two nodes are labeled by the same values and same signs.
~
### Integer linear arithmetic

The mixed integer linear solver comprises of several layers. It contains several substantial improvements
over Z3's original arithmetic solver and currently outperforms the legacy solver on the main SMTLIB benchmark
sets, and to our knowledge mostly on user scenarios.

#### GCD consistency
Each row is first normalized by multiplying the least common multiple of the denominators of each coefficient. 
For each row it assembles a value from the fixed variables. 
A variable $x_j$ is fixed if the current values $lo_j$, $hi_j$ are equal. 
Then it checks that the gcd of the coefficients to variables divide the fixed value. If they don't the row has no integer solution.

#### Patching
Following [@MouraB08], the integer solver moves non-basic variables away from their bounds in order to ensure that
basic, integer, variables are assigned integer values. The process examines each non-basic variable and checks every row where it occurs
to estimate a safe zone where its value can be changed without breaking any bounds. If the safe zone is sufficiently large to patch
a basic integer variable it performs an update. This heuristic is highly incomplete, but is able to locally patch several variables
without resorting to more expensive analyses.

#### Cubes
One of the deciding factors in leapfrogging the previous solver relied on 
a method by Bromberger and Weidenbach [@BrombergerW16;@BrombergerW17].
It allows to detect feasible inequalities over integer variables 
by solving a stronger linear system.
In addition, we observed that the default strengthening proposed by Bromberger and Weidenbach
can often be avoided: integer solutions can be guaranteed from weaker systems.

We will here recall the main method and our twist.
In the following we let $A, A'$ range over integer matrices 
and $a$, $b$, $c$ over integer vectors. The 1-norm $\onenorm{A}$ of a matrix
is a column vector, such that each entry $i$ is the sum of the absolute
values of the elements in the corresponding row $A_i$. We write
$\onenorm{A_i}$ to directly access the 1-norm of a row.

A (unit) _cube_ is a polyhedron that is a Cartesian 
product of intervals of length one for each variable.
Since each variable therefore contains an integer point, the interior of the polyhedron
contains an integer point. The condition for a convex polyhedron to contain a cube can be
recast as follows:


~Example
Suppose we have $3x + y \leq 9 \wedge - 3y \leq -2$ and wish to find an integer solution. 
By solving $3x + y \leq 9 - \frac{1}{2}(3 + 1) = 7, -3y \leq -2 - \frac{1}{2}3 = -3.5$ we find
a model where $y = \frac{7}{6}, x = 0$. After rounding $y$ to $1$ and maintaining $x$ at $0$ we obtain an
integer solution to the original inequalities.
~

Our twist on Bromberger and Weidenbach's method is to avoid strengthening on selected inequalities.
First we note that _difference_ inequalities of the form $x - y \leq k$, where $x, y$ are integer variables
and $k$ is an integer offset need not be strengthened. For octagon constraints $\pm x \pm y \leq k$
there is a boundary condition: they need only require strengthening if $x, y$ are assigned at mid-points
between integral solutions. For example, if $V(x) = \frac{1}{2}$ and $V(y) = \frac{3}{2}$, for $x + y \leq 2$.
Our approach is described in detail in [@Vampire17Theorem].


#### Branching
Similar to traditional MIP branch-and-bound methods, 
the solver creates somewhat eagerly case splits on bounds 
of integer variables if the dual simplex solver fails to assign them integer values.


#### Gomory and Hermite cuts
The arithmetic solver produces Gomory cuts from rows where the basic variables are non-integers after
the non-basic variables have been pushed to the bounds. It also incorporates algorithms from [@DilligDA09;@ChristH15]
to generate cuts after the linear systems have been transformed
into Hermite matrices.






### Non-linear arithmetic

Similar to solving for integer feasibility, the arithmetic solver
solves constraints over polynomials using a waterfall model for non-linear
constraints.
At the basis it maintains for every monomial 
term $x \cdot x \cdot y$ an equation
$m := x \cdot x \cdot y$, where $m$ is a variable
that represents the monomial $x \cdot x \cdot y $.
The module for non-linear arithmetic then attempts to 
establish a valuation $V$ where $V(m) = V(x) \cdot V(x) \cdot V(y)$, 
or derive a consequence that no such valuation exists.
The stages in the waterfall model are summarized as follows:


#### Bounds propagation on monomials
A relatively inexpensive step is to propagate and check bounds based on 
on non-linear constraints. For example, for $y \geq 3$, then $m := x\cdot x\cdot y \geq 3$,
if furthermore $x \leq -2$, we have the strengthened bound $m \geq 12$.
Bounds propagation can also flow from bounds on $m$ to bounds on the 
variables that make up the monomial, such that when $m \geq 8, 1 \leq y \leq 2, x \leq 0$, 
then we learn the stronger bound $x \leq -2$ on $x$.

#### Bounds propagation with Horner expansions
If $x \geq 2, y \geq -1, z \geq 2$, then $y + z \geq 1$ 
and therefore $x\cdot (y + z) \geq 2$, but we would not be
able to deduce this fact if combining bounds individually for $x\cdot y$ 
and $x \cdot z$ because no bounds can be inferred for $x \cdot y$ in isolation.
The solver therefore attempts different re-distribution of multiplication
in an effort to find stronger bounds.

#### Gr&ouml;bner reduction
We use an adaptation of ZDD (Zero suppressed decision diagrams [@Minato93;@NishinoYMN16]) to represent polynomials.
The representation has the advantage that polynomials are stored in a shared data-structure and operations
over polynomials are memoized. A polynomial over the real is represented as an acyclic graph where 
nodes are labeled by variables and edges are labeled by coefficients. For example, the polynomial $5x^2y + xy + y + x + 1$
is represented by the acyclic graph shown below.

~ Snippet

\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\tikzstyle{block} = [rectangle, draw, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']

\begin{tikzpicture}[node distance = 3em, scale = 0.2]
  \node [circle, draw] (xroot) {$x$};
  \node [circle, below of = xroot, left of= xroot, draw] (xnext) {$x$};
  \node [circle, below of = xnext, left of= xnext, draw] (y1) {$y$};
  \node [circle, below of = xnext, right of= xnext, draw] (y2) {$y$};
  \node [circle, below of = y1, left of = y1] (t1) {$5$};
  \node [circle, below of = y1, right of = y1] (t2) {$0$};
  \node [circle, right of = t2] (t3) {$1$};
  \node [circle, below of = y2, right of = y2] (t4) {$1$};
  \path [line] (xroot.west) -- (xnext.north);
  \path [line] (xroot.east) -- (y2.north);
  \path [line] (xnext.west) -- (y1.north);
  \path [line] (xnext.east) -- (y2.north);
  \path [line] (y1.west) -- (t1.north);
  \path [line] (y1.east) -- (t2.north);
  \path [line] (y2.west) -- (t3.north);
  \path [line] (y2.east) -- (t4.north);

\end{tikzpicture}
~

The root node labeled by $x$ represents the polynomial $x\cdot l + r$, 
where $l$ is the polynomial of the left sub-graph and $r$ the polynomial
of the right sub-graph. The left sub-graph is allowed to be labeled again by $x$, 
but the rigth sub-graph may only have nodes labeled by variables that are smaller
in a fixed ordering. The fixed ordering used in this example sets $x$ above $y$.
Then the polynomial for the right sub-graph is $y + 1$, and the polynomial with the
left subgraph is $5xy + (y + 1)$.

The Gr&ouml;bner module performs a set of partial completion steps, preferring
to eliminate variables that can be isolated, and expanding a bounded number of super-position
steps.

#### Incremental linearization
Following [@CimattiGIRS18] we incrementally linearize monomial constraints. 
For example, we include lemmas of the form $x = 0 \rightarrow m = 0$
and $x = 1 \rightarrow m = y$, for $m = x^2y$. 

#### NLSat
As an end-game attempt, the solver attempts to solver the non-linear constraints using a complete solver
for Tarski's fragment supported by the NLSat solver [@JovanovicM12].

# Reducible Theories

## Refinement Types
Let us illustrate a use of _reduction_ from richer theories to base theories based on 
a simple example based on refinement types. 
It encodes refinement types using auxiliary functions as explained
in [@BartJacobsCategoricalLogicAndTypeTheory]. Abstractly, a refinement
type of sort $S$ uses a predicate $p$ over $S$. At least one element of $S$ must satisfy $p$ for the
construction to make sense. The refinement type $S \mid p$ represents the elements of $S$ that satisfy
$p$. The properties we need to know about elements of $S\mid p$ can be encoded using two auxiliary
functions that form a surjection $\restrictOp$ from $S \mid p$ into $S$ with a partial inverse $\restrictOp$ that maps
elements from $S$ into $S \mid p$. The properties of these functions are summarized as follows:

~MathPre
  p : S \rightarrow Bool
  \relaxOp : S \mid p \rightarrow S
  \restrictOp : S \rightarrow S \mid p
  \forall x : S \mid p \ . \ \restrictOp(\relaxOp(x)) = x
  \forall s : S \ . \ p(s)\ \rightarrow \ \relaxOp(\restrictOp(s)) = s
  \forall x : S \mid p \ . \ p(\relaxOp(s))
~

Let us illustrate the sort of natural numbers as a refinement type of integers:

~Example

~~MathPre
  sort Nat = Int \mid \lambda x \ . \ x \geq 0
  \forall n : Nat \ . \ \restrictOp(\relaxOp(n)) = n \wedge \relaxOp(n) \geq 0
  \forall i : Int \ . \ i \geq 0 \rightarrow \relaxOp(\restrictOp(i)) = i
~~

~

We obtain a theory solver for formulas with refinement types by instantiating these axioms whenever there is a term $t$ introduced
of sort $S \mid p$ introduced as part of the input or during search (from instantiating quantifiers).
The main challenge with supporting this theory is to ensure that the new terms introduced from axiom instantiation 
is bounded. We don't want the solver to create terms $\relaxOp(\restrictOp(\relaxOp(\restrictOp(\ldots))))$.


* For every sub-term of the form $\restrictOp(t)$, where $t$ is not $\relaxOp(t')$ instantiate the axiom:
  * $p(t) \Rightarrow \relaxOp(\restrictOp(t)) = t$

* For every term $t$ of sort $S \mid p$ instantiate the axioms:
    * $\restrictOp(\relaxOp(t)) = t$
    * $p(\relaxOp(t))$ 

## Reducible theories in Z3

### Arrays

Z3 reduces the theory of arrays to reasoning about uninterpreted functions.
It furthermore treats arrays as function spaces. The first-order theory
of arrays enjoys compactness and so the following formula is satisfiable[^jazzman]

[^jazzman]: thanks to Jasmin Blanchette for drawing attention to this distinction.

~Math
\forall a : Array(Int, Int) \ . \ \exists k \ . \ \forall i \geq k \ . \ a[i] = 0.
~

The same formula is not satisfiable when arrays range over function spaces.
The distinction is only relevant for formulas that contain quantifiers over arrays.

The central functionality of the decision procedure for arrays is to ensure
that a satisfying model under the theory of EUF translates to a satisfying
model in the theory of arrays. To this end, the main service of the theory
solver is to saturate the search state with $\beta$ reduction axioms
for array terms that admit beta-reduction. We call these terms $\lambda$ terms
and they are defined by the beta-reduction axioms:
~MathPre
\beta(\mathit{Store}(A,j,v)[i]) & = & \mathit{if}\ i = j\ \mathit{then} \ v \ \mathit{else}\ A[i]
\beta(\mathit{Map}(f,A,B)[i]) & = & f(A[i],B[i])
\beta(\mathit{AsArray}(f)[i]) & = & f(i)
\beta(\mathit{Const}(v)[i]) & = & v
\beta((\mathit{Lambda}(x, M))[i]) & = & M[i/x]
~

The reduction into EUF, is then in a nutshell an application of the following inference rule:

~Math
\AxiomC{$b$ is a lambda term}
\AxiomC{$a[j]$ is a term}
\AxiomC{$b \sim a$ ($a, b$ are equal under EUF)}
\TrinaryInfC{$b[j] = \beta(b[j])$}
\DisplayProof
~


### Floating points
Floating point semantics can be defined in terms of bit-vector operations. The solver for floating points
uses this connection to reduce the theory of floating points to bit-vectors.

### Algebraic Datatypes
The theory of algebraic datatypes is compiled into uninterpreted functions. 
In this theory, constructors are injective functions. Injectivity is ensured
by adding axioms for partial inverses. For the example of LISP S-expressions
these axioms are: 
~Math
  car(cons(x,y)) = x, cdr(cons(x,y)) = y
~
The main functionality provided by the theory solver that cannot be reduced to EUF 
is the _occurs check_.

### Special Relations

# Hybrid Theories
The prime example of a hybrid theory in Z3 is the theory of strings, regular expressions and sequences.

The theory of strings and regular expressions has entered mainstream SMT solving
thanks to community efforts around standardization and solvers. The SMTLIB2 format
for unicode strings [@SMTUnicode] ...

* Describe skolem function idea.
* Describe normalization
* Describe waterfall
* Regular expression solving

# External Theories

* Add an example, where the Pseudo-Boolean constraint, if written
directly, is enormous, but maintaining it incrementally is cheap.

```python
   f = Function('f', BitVec(10), BitVec(10))
   g = Function('g', BitVec(10), BitVec(10))

   Sum(i*j*(f[i]=i,g[j]=j)) <= 100
```

```python

class PropagateInequality(UserPropagate):
    def __init__(self, s):
        super(self.__class__, self).__init__(s)
        self.add_fixed(self.is_fixed)
        self.add_final(self.final_check)
        self.add_eq(self._add_eq)
        self.add_diseq(self._add_diseq)

    #override
    def push(self):
        print("push")

    #override
    def pop(self, num_scopes):
        print("pop", num_scopes)
    
    def is_fixed(self, id, e):
        print("fixed", id, e)

    def final_check(self):
        print("final")

    def _add_eq(self, x, y):
        pass

    def _add_diseq(self, x, y):
        pass

    # TBD test
    def fresh(self, new_ctx):        
        return PropagateInequality(s)
        
```

~Example 
Pseudo Booleans

~

# Extensions

* In-processing
* Induction Solvers
* Word-level bit-vectors
* Combination of MBQI and Quantifier Projection. Model-based projection.

* Model counting

* Interpolation

# A modernized architecture { #sec-new-arch }

* Where can benefits from one solver be imported into the main soup.
* MCSAT promises and challenges
* 

[BIB]
